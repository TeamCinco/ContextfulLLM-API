[storage]
documents_path = documents/
cached_prompts_path = cached_prompts/

; Use OpenAI client for now
[inference provider]
base_url = https://api.groq.com/openai/v1

; Supply your arguments for your inference provider here
[infernce arguments]
model = llama-3.1-70b-versatile
temperature = 0.5
max_tokens = 8000

[other_settings]
verbosity = False
cache_prompts = False ;Not used for now
