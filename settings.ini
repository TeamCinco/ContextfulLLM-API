[storage]
documents_path = documents/
cached_prompts_path = cached_prompts/

; Use OpenAI client for now
[inference provider]
base_url = https://openrouter.ai/api/v1

; Supply your arguments for your inference provider here
[infernce arguments]
model = meta-llama/llama-3.1-70b-instruct
temperature = 0.5

;Not used for now
[other_settings]
verbosity = False
cache_prompts = False 


; ; Use OpenAI client for now
; [inference provider]
; base_url = https://api.groq.com/openai/v1

; ; Supply your arguments for your inference provider here
; [infernce arguments]
; model = llama-3.1-70b-versatile
; temperature = 0.5
; max_tokens = 8000